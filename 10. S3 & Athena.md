# S3 & Athena
* ## MFA Delete
  * Only the bucket owner (root account) can enable/disable MFA-Delete
  * MFA-Delete currently can only be enabled using the CLI
* ## Access Logs
  if you want to create one bucket as logs you need to create one bucket just for logging and then enabled Access Loggins for other bucket and show log bucket as your logs-bucket
  * Amazon Athena can be used for analyze logs
  * Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
  * Do not set your logging bucket to be the monitored bucket
  * It will create a logging loop, and your bucket will grow in size exponentially
* ## S3 Replication
  * Must enable versioning in source and destination
  * Cross Region Replication (CRR)
  * Same Region Replication (SRR)
  * Buckets can be in different accounts
  * Copying is asynchronous
  * Must give proper IAM permissions to S3
  * CRR - Use cases: compliance, lower latency access, replication across accounts
  * SRR – Use cases: log aggregation, live replication between production and test accounts
  * After activating, only new objects are replicated (not retroactive)
  There is no “chaining” of replication
  * If bucket 1 has replication into bucket 2, which has replication into bucket 3
  * Then objects created in bucket 1 are not replicated to bucket 3
* ## S3 Pre-Signed URLs
  here we are going to create a public url for our object for specific time. especially when you use CLI you need to create 
  pre-signed-url for access object.
  * Allow only logged-in users to download a premium video on your S3 bucket
  * Allow an ever changing list of users to download files by generating URLs dynamically
  * Allow temporarily a user to upload a file to a precise location in our bucket
  * Can generate pre-signed URLs using SDK or CLI
  * Valid for a default of 3600 seconds, can change timeout with --expires-in [TIME_BY_SECONDS] argument
  * Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT
* ## S3 Storage Classes
1. ### S3 Standart
    * 99.99% Availability over a given year
    * Use Cases: Big Data analytics, mobile & gaming applications, content distribution… 
    * Sustain 2 concurrent facility failures
2. ### S3 Standard – Infrequent Access
    * Suitable for data that is less frequently accessed, but requires rapid access when needed
    * 99.9% Availability
    * Low cost compared to Amazon S3 Standard
    * Sustain 2 concurrent facility failures
    * Use Cases: As a data store for disaster recovery, backups
3. ### S3 One Zone - Infrequent Access
    * Same as IA but data is stored in a single AZ
    * 99.5% Availability
    * Low latency and high throughput performance
    * Supports SSL for data at transit and encryption at rest
    * Low cost compared to IA (by 20%)
    * Use Cases: Storing secondary backup copies of on-premise data, or storing data you can recreate
4. ### S3 Intelligent Tiering
    * Same low latency and high throughput performance of S3 Standard
    * Small monthly monitoring and auto-tiering fee
    * Automatically moves objects between two access tiers based on changing access patterns
    * Designed for durability of 99.999999999% of objects across multiple Availability Zones
    * Resilient against events that impact an entire Availability Zone
    * Designed for 99.9% availability over a given year
5. ### Amazon Glacier
    * Low cost object storage meant for archiving / backup 
    * Data is retained for the longer term (10s of years) 
    * Alternative to on-premise magnetic tape storage 
    * Average annual durability is 99.999999999% 
    * Cost per storage per month ($0.004 / GB) + retrieval cost 
    * Each item in Glacier is called “Archive” (up to 40TB) 
    * Archives are stored in ”Vaults”
6. ### Amazon Glacier & Glacier Deep Archive
    * Amazon Glacier – 3 retrieval options:
      * Expedited (1 to 5 minutes)
      * Standard (3 to 5 hours)
      * Bulk (5 to 12 hours)
      * Minimum storage duration of 90 days
    * Amazon Glacier Deep Archive – for long term storage – cheaper:
      * Standard (12 hours)
      * Bulk (48 hours)
      * Minimum storage duration of 180 days  
7. ### S3– Moving between storage classes
  * Moving objects can be automated using a lifecycle configuration
    ![S3-STORAGE-LIFECYCLE](/images/S3-STORAGE-LIFECYCLE.PNG "S3-STORAGE-LIFECYCLE")
8. ### S3 Lifecycle Rules
  * Transition actions: It defines when objects are transitioned to another storage class
    * Move objects to Standard IA class 60 days after creation
    * Move to Glacier for archiving after 6 months   
  * Expiration actions: configure objects to expire (delete) after some time
  * Access log files can be set to delete after a 365 days
    * Can be used to delete old versions of files (if versioning is enabled)
    * Can be used to delete incomplete multi-part uploads
    * Rules can be created for a certain prefix (ex - s3://mybucket/mp3/*)
    * Rules can be created for certain objects tags (ex - Department: Finance)
9. ### S3 – KMS Limitation
   * If you use SSE-KMS, you may be impacted by the KMS limits
   * When you upload, it calls the GenerateDataKey KMS API
   * When you download, it calls the Decrypt KMS API

10. ### S3 Performance
    * Multi-Part upload:
      * recommended for files > 100MB, must use for files > 5GB
      * Can help parallelize uploads (speed up transfers)
    * S3 Transfer Acceleration (upload only)
      * Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region
      * Compatible with multi-part upload
11. ### S3 Select & Glacier Select
    * Retrieve less data using SQL by performing server side filtering
    * Can filter by rows & columns (simple SQL statements)
    * Less network transfer, less CPU cost client-side
12. ### S3 Event Notifications
    * S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer
    * If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent
    * If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.
13. AWS Athena
    * Serverless service to perform analytics directly against S3 files
    * Uses SQL language to query the files
    * Has a JDBC / ODBC driver
    * Charged per query and amount of data scanned
    * Supports CSV, JSON, ORC, Avro, and Parquet (built on Presto)
    * Use cases: Business intelligence / analytics / reporting, analyze & query VPC Flow Logs, ELB Logs, CloudTrail trails, etc...
    * Exam Tip: Analyze data directly on S3 => use Athena